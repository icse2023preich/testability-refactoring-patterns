{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "from utils import *\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "IMAGES_PATH='images-2022/'\n",
    "latex_vars=dict()\n",
    "tp_pr_name='Test-Pairs PRs'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_project(x):\n",
    "    return '/'.join(x.split('/')[3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr=pd.read_csv('new_pullreq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how have we derived projects.csv?\n",
    "projects=pd.read_csv('projects.csv')\n",
    "allowed_projects=projects[projects.source=='new_pullreq.csv']['project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 s, sys: 3.78 s, total: 27.8 s\n",
      "Wall time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TODO: how have we derived all_prs_with_files?\n",
    "all_prs_with_files=pd.read_csv('all_prs_with_files.csv')\n",
    "if not os.path.isfile('cached_all_prs.csv'):\n",
    "    all_prs=generate_all_prs(all_prs_with_files)\n",
    "    all_prs['selected']='Other'\n",
    "    all_prs.loc[(all_prs['test_pairs'] > 0) & (all_prs['changed_files'] < 10),'selected']='small_with_tpairs'\n",
    "    all_prs.loc[~(all_prs['test_pairs'] > 0) & (all_prs['changed_files'] < 10),'selected']='small_without_tpairs'\n",
    "    all_prs.loc[(all_prs['test_pairs'] > 0) & ~(all_prs['changed_files'] < 10),'selected']='large_with_tpairs'\n",
    "    all_prs.loc[~(all_prs['test_pairs'] > 0) & ~(all_prs['changed_files'] < 10),'selected']='large_without_tpairs'\n",
    "    all_prs['churn']=all_prs['prod_additions']+all_prs['prod_deletions']+all_prs['test_additions']+all_prs['test_deletions']\n",
    "    all_prs[all_prs.selected.isin(['small_with_tpairs','large_with_tpairs'])][['churn','changed_files','test_pairs']].quantile([0.1,0.5,0.6,0.62,0.9])\n",
    "    all_prs['title_mask']=all_prs.title.apply(lambda x: is_testability_relevant(x)[1])\n",
    "    all_prs.to_csv('cached_all_prs.csv',index=False)\n",
    "else:\n",
    "    all_prs=pd.read_csv('cached_all_prs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "842723"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_prs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prs['project']=all_prs['url'].apply(extract_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prs_raw=all_prs\n",
    "all_prs=all_prs[all_prs['project'].isin(allowed_projects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_agreement_rates(m, key1='pr_group_primary',key2='pr_group_secondary'):\n",
    "    m.loc[m[key1].isnull(),key1]='irrelevant'\n",
    "    m.loc[m[key2].isnull(),key2]='irrelevant'\n",
    "    print(cohen_kappa_score(m[key1],m[key2]))\n",
    "    d=pd.crosstab(m[key1],m[key2],margins=True)\n",
    "    print(df_to_latex(d))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_reviewed_raw=pd.read_csv('reviewed.csv')\n",
    "manually_reviewed=manually_reviewed_raw\n",
    "primary_reviewed=pd.read_csv('primary_reviewed.csv')\n",
    "secondary_reviewed=pd.read_csv('secondary_reviewed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive projects\n",
    "manually_reviewed['project']=manually_reviewed['url'].apply(extract_project)\n",
    "primary_reviewed['project']=primary_reviewed['url'].apply(extract_project)\n",
    "secondary_reviewed['project']=secondary_reviewed['url'].apply(extract_project)\n",
    "#take projects from new_pullreq.csv only\n",
    "manually_reviewed=manually_reviewed[manually_reviewed.project.isin(allowed_projects)]\n",
    "primary_reviewed=primary_reviewed[primary_reviewed.project.isin(allowed_projects)]\n",
    "secondary_reviewed=secondary_reviewed[secondary_reviewed.project.isin(allowed_projects)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to 50 PRs per masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(manually_reviewed.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test                 162\n",
       "Other                152\n",
       "testability_body     114\n",
       "Refactor for test     91\n",
       "Dependency            79\n",
       "Concurrency           76\n",
       "Inject                76\n",
       "Network               62\n",
       "Singleton             24\n",
       "testability           21\n",
       "Name: title_mask, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manually_reviewed.title_mask.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency 0\n",
      "Concurrency 0\n",
      "Network 5\n",
      "Inject 0\n"
     ]
    }
   ],
   "source": [
    "masks_to_downsample=['Dependency','Concurrency','Network','Inject']\n",
    "downsample_limit=50\n",
    "df=manually_reviewed[~manually_reviewed.title_mask.isin(masks_to_downsample)]\n",
    "for mask in masks_to_downsample:\n",
    "    urls=manually_reviewed[manually_reviewed.title_mask==mask]['url'].drop_duplicates()\n",
    "    durls=manually_reviewed[(manually_reviewed.title_mask==mask) & \n",
    "                           (manually_reviewed.url.isin(secondary_reviewed.url))]['url'].drop_duplicates()\n",
    "    resturls=urls[~urls.isin(durls)]\n",
    "    if len(durls)>downsample_limit:\n",
    "        durls=durls.sample(n=downsample_limit, random_state=42)\n",
    "    n=downsample_limit-len(durls)\n",
    "    print(mask, n)\n",
    "    dr=manually_reviewed[manually_reviewed.url.isin(durls)]\n",
    "    sr=manually_reviewed[manually_reviewed.url.isin(resturls.sample(n=n, random_state=42))]\n",
    "    df=df.append(dr).append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other                150\n",
       "test                 150\n",
       "testability_body     109\n",
       "Refactor for test     73\n",
       "Dependency            50\n",
       "Concurrency           50\n",
       "Network               50\n",
       "Inject                50\n",
       "Singleton             24\n",
       "testability           18\n",
       "Name: title_mask, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['title_mask','url']].drop_duplicates().title_mask.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_reviewed=manually_reviewed[manually_reviewed.url.isin(df.url)]\n",
    "primary_reviewed=primary_reviewed[primary_reviewed.url.isin(df.url)]\n",
    "secondary_reviewed=secondary_reviewed[secondary_reviewed.url.isin(df.url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of PRs to review\n",
    "upsample_masks=['Other','test']\n",
    "prs_to_sample=pd.DataFrame()\n",
    "upsample_limit=100\n",
    "for mask in upsample_masks:\n",
    "    sel=all_prs[~(all_prs.url.isin(manually_reviewed_raw.url)) & \\\n",
    "        (all_prs.java_files_count < 10) & (all_prs.test_pairs > 0)  \\\n",
    "        & (all_prs.title_mask==mask)]\n",
    "    already_asked=pd.read_csv('aug2022_'+mask+'.csv')\n",
    "    sel=sel[~sel.url.isin(already_asked.url)]\n",
    "    left_to_sample=upsample_limit-len(set(manually_reviewed[manually_reviewed.title_mask==mask].url))\n",
    "    left_to_sample=50\n",
    "    mask_sel=sel.sample(n=left_to_sample, random_state=42)\n",
    "    mask_sel[['url','title_mask']].to_csv('sample_16aug2022_' + mask + '.csv', index=False)\n",
    "    prs_to_sample=prs_to_sample.append(mask_sel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other    50\n",
       "test     50\n",
       "Name: title_mask, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prs_to_sample.title_mask.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testable_prs=pd.read_csv('testable_prs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=primary_reviewed[['url','pr_group','period']].drop_duplicates().\\\n",
    "    merge(secondary_reviewed[['url','pr_group','period']].drop_duplicates(), on=['url','period'],suffixes=['_primary','_secondary'])\n",
    "m['relevant_primary']=m['pr_group_primary']!='irrelevant'\n",
    "m['relevant_secondary']=m['pr_group_secondary']!='irrelevant'\n",
    "latex_vars['prgrouptwokappa']=round(cohen_kappa_score(m['relevant_primary'],m['relevant_secondary']),3)\n",
    "latex_vars['prgroupthreekappa']=round(cohen_kappa_score(m['pr_group_primary'],m['pr_group_secondary']),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pattern_match(a,b):\n",
    "    if set(a)==set(b):\n",
    "        return 'equal matches'\n",
    "\n",
    "    common=set(a).intersection(set(b))\n",
    "    if len(common) > 0:\n",
    "        return 'partial matches'\n",
    "    return 'no matches'\n",
    "m['match']=m['url'].apply(lambda u: \\\n",
    "    calc_pattern_match(primary_reviewed[primary_reviewed.url==u]['ref_pattern'], \\\n",
    "                       secondary_reviewed[secondary_reviewed.url==u]['ref_pattern']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 5\n",
    "by_equality=m.match.value_counts()\n",
    "by_equality['total']=by_equality.sum()\n",
    "by_equality=pd.DataFrame(by_equality).reset_index()\n",
    "by_equality['name']=by_equality.index\n",
    "by_equality_s=by_equality[['index','match']].to_latex(index=False,header=['Agreement level','PR count'])\n",
    "by_equality_s=by_equality_s.replace('toprule','hline').replace('bottomrule','hline')\n",
    "by_equality_l=by_equality_s.split('\\n')\n",
    "by_equality_s='\\n'.join(by_equality_l[:-4])+'\\n\\hline\\n'+'\\n'.join(by_equality_l[-4:])\n",
    "print(by_equality_s)\n",
    "open('table_two_coders_agreement.tex','w').write(by_equality_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate kappa for PRs with only one pattern\n",
    "if True:\n",
    "    single_urls=manually_reviewed.url.value_counts()\n",
    "    single_urls=single_urls[single_urls==1].index\n",
    "    rpm=primary_reviewed[primary_reviewed.url.isin(single_urls)][['url','pr_group','ref_pattern']].\\\n",
    "        merge(secondary_reviewed[secondary_reviewed.url.isin(single_urls)][['url','pr_group','ref_pattern','comment','fauthor_comment','reviewer']],on='url',suffixes=['_primary','_secondary'])\n",
    "#    calc_agreement_rates(rpm,'pr_group_primary','pr_group_secondary')\n",
    "#    calc_agreement_rates(rpm,'ref_pattern_primary','ref_pattern_secondary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not so sure manually_reviewed.loc[manually_reviewed['title_mask']=='testability_body','title_mask']='testability'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_by_url=manually_reviewed[['url','pr_group','title_mask']].drop_duplicates()\n",
    "result_by_url=result_by_url[result_by_url.title_mask.notnull()]\n",
    "result_by_url['doublechecked']=result_by_url['url'].isin(secondary_reviewed.url)\n",
    "#result_by_url=result_by_url.merge(all_prs[['url','title_mask']].drop_duplicates())\n",
    "result_by_url['project']=result_by_url['url'].apply(get_project)\n",
    "\n",
    "latex_vars['manuallyreviewedprcount']=len(set(result_by_url['url']))\n",
    "latex_vars['testabilityrelevantprcount']=len(set(result_by_url[result_by_url['pr_group']!='irrelevant']['url']))\n",
    "latex_vars['testabilityrelevantoccurrencecount']=len(manually_reviewed[manually_reviewed['pr_group']!='irrelevant'])\n",
    "print('We have manually reviewed %d pull requests from %d open source java projects and in %d of them we have identified refactoring patterns related to unit-tests' % \\\n",
    "      (latex_vars['manuallyreviewedprcount'], len(set(result_by_url['project'])), \n",
    "       latex_vars['testabilityrelevantprcount']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_reviewed[manually_reviewed['pr_group']=='other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['nprcount']=len(npr)\n",
    "latex_vars['nprcount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['allprscount']=len(all_prs)\n",
    "latex_vars['allprsonejavacount']=len(all_prs[all_prs['java_files_count'] > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['tpprcount']=len(all_prs[(all_prs['test_pairs']>0) & (all_prs['java_files_count'] > 0)])\n",
    "latex_vars['otherprcount']=len(all_prs[(all_prs['test_pairs']==0) & (all_prs['java_files_count'] > 0)])\n",
    "(latex_vars['tpprcount'],latex_vars['otherprcount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['tpprtencount']=len(all_prs.query('test_pairs > 0 & changed_files < 10'))\n",
    "latex_vars['tpprtencount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp=npr[npr['language']=='Java']\n",
    "jp['project']=jp[['ownername','reponame']].apply(lambda xs: xs['ownername'] + \"/\" + xs['reponame'], axis=1)\n",
    "top_projects=jp[jp['core_member']==0][['project','merged_or_not']].groupby(by=['project']).count()\n",
    "top_projects['cnt'] = top_projects['merged_or_not']\n",
    "top_projects['project']=top_projects.index\n",
    "top_projects=top_projects[top_projects['cnt'] > 50]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['javaprojcount']=len(set(jp['project']))\n",
    "latex_vars['javaprojcount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['javaprojselectedcount']=len(set(top_projects['project']))\n",
    "latex_vars['javaprojselectedcount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['allprsprojcount']=len(set(all_prs['project']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['javaprojcount']=len(set(jp['project']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_30k_mined=pd.read_csv('cached_30k_mined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars-summary\n",
    "# all-prs - summary of PRs, derived from .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mined_30k=pd.read_csv('prs_to_sample2.csv')\n",
    "mined_30k=mined_30k.merge(all_prs[['selected','url','title_mask','test_pairs']].drop_duplicates())\n",
    "# mined random sample rq1\n",
    "# take only allowed projects\n",
    "mined_30k['project']=mined_30k['url'].apply(extract_project)\n",
    "mined_30k=mined_30k[mined_30k['project'].isin(allowed_projects)]\n",
    "len(mined_30k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['minedprojectscount']=len(set(mined_30k['url'].apply(get_project)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['minedprcount']=len(set(mined_30k['url']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_30k.loc[mined_30k.url.isin(manually_reviewed[manually_reviewed.title_mask=='testability_body']['url']),\\\n",
    "              'title_mask']='testability_body'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_to_latex(df, header=False,index=True,formatters=None):\n",
    "    lines=df.to_latex(header=header,index=index,formatters=formatters).split('\\n')\n",
    "\n",
    "    lines=[line for line in lines if not re.match(r'\\\\begin|\\\\end|\\\\toprule|\\\\bottomrule', line)]\n",
    "    lines=['\\\\rowcolor{palegrey}' + x[0]  if x[1] % 2 == 0 else x[0] for x in zip(lines,range(len(lines)))]\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_mr=manually_reviewed[['url','title_mask']].drop_duplicates()\n",
    "t2_retrieved=(t2_mr.append(all_prs[all_prs['selected']=='small_with_tpairs'][['url','title_mask']])).drop_duplicates()['title_mask'].value_counts()\n",
    "t2_sampled=(t2_mr.append(manually_reviewed[['url','title_mask']])).drop_duplicates()['title_mask'].value_counts()\n",
    "t2_mined=mined_30k[mined_30k['test_pairs'] >0][['url','title_mask']].drop_duplicates()['title_mask'].value_counts()\n",
    "#t2_mined=(t2_mr.append(mined_30k[mined_30k['test_pairs'] >0][['url','title_mask']])).drop_duplicates()['title_mask'].value_counts()\n",
    "table2=pd.DataFrame(t2_retrieved).reset_index().rename(columns={'title_mask':'retrieved'})\n",
    "table2=table2.merge(pd.DataFrame(t2_mined).reset_index().rename(columns={'title_mask':'mined'}), how='left')\n",
    "table2=table2.merge(pd.DataFrame(t2_sampled).reset_index().rename(columns={'title_mask':'sampled'}))\n",
    "table2=table2.rename(columns={'index':'title_mask'})\n",
    "table2['mined']=table2['mined'].fillna(0)\n",
    "table2['retrieved_pct']=round(table2['retrieved']/sum(table2['retrieved'])*100,1)\n",
    "table2['mined_pct']=round(table2['mined']/sum(table2['mined'])*100,1)\n",
    "table2['sampled_pct']=round(table2['sampled']/sum(table2['sampled'])*100,1)\n",
    "table2['mined']=table2['mined'].astype(int)\n",
    "table2=table2[['title_mask','mined','mined_pct','retrieved','sampled','sampled_pct']]\n",
    "table2.columns = pd.MultiIndex.from_tuples([('title_mask','title_mask'),\n",
    "                                            ('RQ1', 'mined'),\n",
    "                                            ('RQ1','mined_pct'),\n",
    "                                            ('RQ2', 'retrieved'),\n",
    "                                            ('RQ2','sampled'),\n",
    "                                            ('RQ2','sampled_pct'),\n",
    "                                           ])\n",
    "title_masks=['testability_body','testability','Refactor for test','Dependency','Concurrency','Network','Singleton','Inject','test','Other']\n",
    "#table2.index=table2.title_mask\n",
    "#\n",
    "#table2[title_masks]\n",
    "#[x for x in table2['title_mask']]\n",
    "table2.index=[x[0] for x in table2['title_mask'].values.tolist()]\n",
    "table2.loc['Total']=table2.drop(columns='title_mask').sum()\n",
    "table2.loc['Total','title_mask']='Total'\n",
    "#table2['retrieved'] = table2['retrieved'].astype(int)\n",
    "latab=table2.reindex(title_masks+['Total']).reset_index(drop=True)\n",
    "def _color_if_even(s):\n",
    "    return ['\\rowcolor{palegrey}' if val % 2 == 0 else '' for val in s]\n",
    "latab.style.applymap(_color_if_even)\n",
    "latab.style.applymap(lambda s: 'banana', subset=[('RQ1','mined')])\n",
    "latab[latab['title_mask']['title_mask']=='Total']=latab[latab['title_mask']['title_mask']=='Total'].round()\n",
    "print(rows_to_latex(latab,index=False, formatters=\\\n",
    "                   {('RQ1','mined'):lambda s: '%d' % s,\\\n",
    "                   ('RQ2','retrieved'):lambda s: '%d' % s,\\\n",
    "                    ('RQ2','sampled'):lambda s: '%d' % s,\\\n",
    "                   }) )\n",
    "latab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "title_masks_words=OrderedDict({\n",
    "    'testability_body':'testability OR testable',\n",
    "    'testability':'testability OR testable',\n",
    "    'Refactor for test':'refactor AND (test OR junit)',\n",
    "    'Dependency':'depend',\n",
    "    'Concurrency':'concurren OR thread OR sleep OR latch',\n",
    "    'Network':'network OR socket OR connectivity OR connection',\n",
    "    'Singleton':'singleton',\n",
    "    'Inject':'inject OR wire OR wiring',\n",
    "    'test':'test OR junit',\n",
    "    'Other':'Anything else'\n",
    "})\n",
    "title_masks_df=pd.DataFrame(title_masks_words.keys(),index=['M'+str(x+1) for x in range(len(title_masks_words))])\n",
    "title_masks_df['ID']=title_masks_df.index\n",
    "title_masks_df['Name']=title_masks_df[0]\n",
    "title_masks_df['Keywords sought for']=title_masks_words.values()\n",
    "latab['Name']=latab[('title_mask','title_mask')]\n",
    "merged_table_title_masks=title_masks_df.merge(latab, left_on='Name', right_on='Name')\n",
    "merged_table_title_masks=merged_table_title_masks.rename(columns=\\\n",
    "    {('RQ2','retrieved'):'Ret.',\n",
    "     ('RQ2','sampled'):'Rev.',\n",
    "     ('RQ2','sampled_pct'):'%'\n",
    "    })[['ID','Name','Keywords sought for','Ret.','Rev.','%']]\n",
    "merged_table_title_masks['Ret.']=merged_table_title_masks['Ret.'].astype(int)\n",
    "merged_table_title_masks['Rev.']=merged_table_title_masks['Rev.'].astype(int)\n",
    "merged_table_title_masks.loc['Total']=merged_table_title_masks[['Ret.','Rev.','%']].sum().round()\n",
    "merged_table_title_masks['Ret.']=merged_table_title_masks['Ret.'].astype(int)\n",
    "merged_table_title_masks['Rev.']=merged_table_title_masks['Rev.'].astype(int)\n",
    "\n",
    "merged_table_title_masks.loc['Total','ID']=''\n",
    "merged_table_title_masks.loc['Total','Name']=''\n",
    "merged_table_title_masks.loc['Total','Keywords sought for']='Total'\n",
    "merged_table_title_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['monecount']=merged_table_title_masks.loc[merged_table_title_masks.ID.isin(['M1']),'Rev.'].sum()\n",
    "latex_vars['mtwocount']=merged_table_title_masks.loc[merged_table_title_masks.ID.isin(['M2']),'Rev.'].sum()\n",
    "latex_vars['mthreecount']=merged_table_title_masks.loc[merged_table_title_masks.ID.isin(['M3']),'Rev.'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['monetwothreecount']=merged_table_title_masks.loc[merged_table_title_masks.ID.isin(['M1','M2','M3']),'Rev.'].sum()\n",
    "latex_vars['mfourtotencount']=merged_table_title_masks.loc[merged_table_title_masks.ID.\\\n",
    "    isin(['M4','M5','M6','M7','M8','M9','M10']),'Rev.'].sum()\n",
    "latex_vars['mfourtotencount']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_to_latex(df, header=False,index=True, grey_idx=0):\n",
    "    lines=df.to_latex(header=header,index=index).split('\\n')\n",
    "\n",
    "    lines=[line for line in lines if not re.match(r'\\\\begin|\\\\end|\\\\toprule|\\\\midrule|\\\\bottomrule', line)]\n",
    "    lines=['\\\\rowcolor{palegrey}' + x[0]  if x[1] % 2 == grey_idx else x[0] for x in zip(lines,range(len(lines)))]\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_table_title_masks_lines=rows_to_latex(merged_table_title_masks, index=False, header=True, grey_idx=1).split('\\n')\n",
    "merged_table_title_masks_hline_idx=[x*2+2 for x in merged_table_title_masks.index[merged_table_title_masks['ID'].isin(['M1','M4','M8','M10'])]]\n",
    "for x in [5,12,15]:\n",
    "    merged_table_title_masks_lines.insert(x, '\\hline')\n",
    "['%d:%s' %p for p in zip(range(len(merged_table_title_masks_lines)),merged_table_title_masks_lines)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_table_title_masks_s='\\n'.join(merged_table_title_masks_lines)\n",
    "merged_table_title_masks_s=merged_table_title_masks_s.replace('midrule','hline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('table_merged_table_title_masks.tex','w').write(merged_table_title_masks_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_reviewed=primary_reviewed.append(secondary_reviewed,sort=True)[['url','reviewer']].drop_duplicates()\n",
    "double_reviewed=double_reviewed.merge(manually_reviewed[['url','title_mask']].drop_duplicates())\n",
    "double_reviewed.reviewer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(manually_reviewed.url)))\n",
    "manually_reviewed['reviewerN']=manually_reviewed['reviewer']\n",
    "sel=double_reviewed[double_reviewed.reviewer!='Reviewer1'].append(manually_reviewed[~manually_reviewed.url.isin(double_reviewed.url)]\\\n",
    "                           [['url','title_mask','reviewer']].drop_duplicates(), sort=True)\n",
    "len(sel)\n",
    "reviewers=pd.crosstab(sel.title_mask,sel.reviewer,margins=True)\n",
    "reviewers\n",
    "print(rows_to_latex(reviewers, header=True, index=True))\n",
    "reviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=df_to_latex(reviewers.reindex(title_masks+['All']),index=True)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel=manually_reviewed[['reviewerN','title_mask','url','pr_group']].drop_duplicates()\n",
    "irr=sel[sel.pr_group=='irrelevant']\n",
    "proportion_irrelevant=100.0*(pd.crosstab(irr.title_mask, irr.reviewerN)/pd.crosstab(sel.title_mask, sel.reviewerN)).round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mined=pd.read_csv('mined30k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mined['project']=rf_mined['url'].apply(extract_project)\n",
    "rf_mined=rf_mined[rf_mined['project'].isin(allowed_projects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mined['with_test_pairs']=rf_mined['test_pairs'].apply(lambda x: x>0)\n",
    "rf_mined[['url','with_test_pairs']].drop_duplicates()['with_test_pairs'].value_counts()\n",
    "sel=rf_mined[rf_mined['with_test_pairs']==True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel=manually_reviewed[['pr_group','url']].drop_duplicates()\n",
    "sel.pr_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_reviewed=manually_reviewed[(manually_reviewed.pr_group!='irrelevant') & (manually_reviewed.ref_pattern!='irrelevant')]\n",
    "patterns=rel_reviewed.rename(columns={'ref_pattern':'pattern'})[['url','pattern']].drop_duplicates()\n",
    "patterns=rel_reviewed.rename(columns={'ref_pattern':'pattern'})[['url','test_location','pattern']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_by_pattern(f):\n",
    "    patterns=rel_reviewed.rename(columns={'ref_pattern':'pattern'})[['url','test_location','pattern']].drop_duplicates()\n",
    "    patterns['pattern']=patterns['pattern'].apply(f)\n",
    "    count_by_pattern=patterns.groupby('pattern').count().reset_index().rename(columns={'url':'count'})[['pattern','count']]\n",
    "    count_by_pattern=count_by_pattern.groupby('pattern').sum().reset_index().sort_values('count',ascending=False)\n",
    "    count_by_pattern=count_by_pattern[count_by_pattern.pattern!='Other'].append(count_by_pattern[count_by_pattern.pattern=='Other'])\n",
    "    count_by_pattern['percentage']=round(100*count_by_pattern['count']/sum(count_by_pattern['count']),1)\n",
    "    count_by_pattern=count_by_pattern[count_by_pattern.pattern!='other'].append(count_by_pattern[count_by_pattern.pattern=='other'])\n",
    "    count_by_pattern.loc['Total']=count_by_pattern.sum()\n",
    "    count_by_pattern.loc['Total','pattern']='Total'\n",
    "    count_by_pattern.loc['Total','percentage']=round(count_by_pattern.loc['Total','percentage'],0)\n",
    "    count_by_pattern['index']=[str(x) for x in range(1,len(count_by_pattern)+1)]\n",
    "    count_by_pattern.loc[count_by_pattern.pattern.isin(['Total','other']),'index']=''\n",
    "    count_by_pattern['index']=count_by_pattern['index'].astype(str)\n",
    "    return count_by_pattern[['index','pattern','count','percentage']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TABLE VII: Frequency of testability refactoring patterns\n",
    "count_by_pattern=summary_by_pattern(lambda s: s)\n",
    "#frequency-patterns\n",
    "count_by_pattern_lines=rows_to_latex(count_by_pattern[['index','pattern','count','percentage']], \\\n",
    "    index=False).split('\\n')\n",
    "\n",
    "count_by_pattern_lines.insert(10,'\\n\\hline')\n",
    "count_by_pattern_lines.insert(12,'\\n\\hline')\n",
    "count_by_pattern_s='\\n'.join(count_by_pattern_lines)\n",
    "open('table_frequency-patterns.tex','w').write(count_by_pattern_s)\n",
    "count_by_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['refpatternscount']=len(count_by_pattern[~count_by_pattern.pattern.isin(['other','Total'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel=manually_reviewed[(manually_reviewed.ref_pattern!='irrelevant') & (manually_reviewed.pr_group!='irrelevant')]\n",
    "sel['relevant']=['testability' if x in ['testability','testability_body'] else 'other' for x in sel.title_mask]\n",
    "p=pd.DataFrame(sel['title_mask'].value_counts())\n",
    "p['title']=p.index\n",
    "p['ntitle']=p[['title','title_mask']].apply(lambda row: row[0] + ' (N=' + str(row[1]) +')',axis=1)\n",
    "p['title_mask']=p.index\n",
    "p[['title_mask','ntitle']]\n",
    "p=p.reindex(title_masks)\n",
    "sel=sel.merge(p,on='title_mask')\n",
    "sam=pd.crosstab(sel.ref_pattern,sel.ntitle).apply(lambda r: 100.0*r/r.sum(),axis=0)\n",
    "image=sns.heatmap(sam[p['ntitle']], annot=True, cbar=False, fmt=\".0f\", cmap=\"YlGn\")\n",
    "plt.ylabel('Refactoring patterns')\n",
    "plt.xlabel('PR masks')\n",
    "plt.xticks(rotation=45,ha='right')\n",
    "image.figure.savefig(IMAGES_PATH+'refpattern-title_masks.eps', transparent=False, bbox_inches='tight')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_base_pattern=summary_by_pattern(lambda s: re.sub(r\"_for_.*\", \"\",s) if isinstance(s,str) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prop_by_pr_group(result_by_url):\n",
    "    m=pd.crosstab(result_by_url.title_mask, result_by_url.pr_group, margins=True).reset_index()\n",
    "    all_prs_by_title_mask=all_prs[all_prs['selected']=='small_with_tpairs'][['title_mask','url']].groupby('title_mask').count().reset_index().rename(columns={'url':'PRs_small_with_tpairs'})\n",
    "    m=m.rename(columns={'All':'manually_reviewed'})\n",
    "    m['irrelevant_ratio']=m['irrelevant']/m['manually_reviewed']\n",
    "    counts_by_masks=m\n",
    "    m=m[m['title_mask'] != 'All'].sort_values('irrelevant_ratio').append(m[m['title_mask']=='All'])\n",
    "    def get_count_with_percent(xs):\n",
    "        return pd.Series([\"%d (%.1f%%)\" % (x, x/sum(xs)*100) for x in xs])\n",
    "    result_types=['incl_ref_for_test','irrelevant','only_ref_for_test']\n",
    "    m[result_types] = m[result_types].apply(get_count_with_percent,axis=1)\n",
    "    m=m.drop(columns=['irrelevant_ratio','manually_reviewed'])\n",
    "    m.index=m['title_mask']\n",
    "    m=m.reindex(title_masks+['All'])\n",
    "    m.loc[m.title_mask=='All','title_mask']='Total (N=' + str(len(set(result_by_url.url)))+')'\n",
    "    m=m[['title_mask','irrelevant','only_ref_for_test','incl_ref_for_test']]\n",
    "    return m\n",
    "table_all_prs_by_mask=calc_prop_by_pr_group(result_by_url)\n",
    "table_all_prs_by_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['percentonlyreffortest'] = round(len(result_by_url[result_by_url.pr_group=='only_ref_for_test']) /\\\n",
    "    len(result_by_url) * 100,2)\n",
    "latex_vars['percentinclreffortest'] = round(len(result_by_url[result_by_url.pr_group=='incl_ref_for_test']) /\\\n",
    "    len(result_by_url) * 100,2)\n",
    "latex_vars['percenttestabilitytitle'] = round(len(result_by_url[\\\n",
    "    (result_by_url.title_mask=='testability') & (result_by_url.pr_group!='irrelevant')]) /\\\n",
    "    len(result_by_url[result_by_url.title_mask=='testability']) * 100,2)\n",
    "latex_vars['percenttestabilitybody'] = round(len(result_by_url[\\\n",
    "    (result_by_url.title_mask=='testability_body') & (result_by_url.pr_group!='irrelevant')]) /\\\n",
    "    len(result_by_url[result_by_url.title_mask=='testability_body']) * 100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_all_prs_by_mask_lines=rows_to_latex(table_all_prs_by_mask, index=False).split('\\n')\n",
    "table_all_prs_by_mask_lines.insert(len(table_all_prs_by_mask_lines)-2,'\\hline')\n",
    "table_all_prs_by_mask_s='\\n'.join(table_all_prs_by_mask_lines)\n",
    "open('table_all_prs_by_mask.tex','w').write(table_all_prs_by_mask_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_reviewed_prs_with_test_pairs(mined, manually_reviewed):\n",
    "    if 'pr_group' in mined:\n",
    "        mined=mined.merge(manually_reviewed[['url','pr_group']].drop_duplicates())\n",
    "        mined['with_test_pairs']=False\n",
    "        mined.loc[mined.pr_group!='irrelevant','with_test_pairs']=True\n",
    "    return mined\n",
    "\n",
    "def exclude_too_large_commits(mined, all_prs_with_files):\n",
    "    # remove commits that are too large and likely to be merge commits\n",
    "    java_files_by_url=all_prs_with_files[all_prs_with_files.changedFile.str.endswith('.java')][['url','changedFile']].groupby('url').count()\n",
    "    cafter_by_url=mined.groupby(['url','commit']).agg({'classesAfter':lambda x: len(set(x))}).sort_values('classesAfter').reset_index()\n",
    "    commits=cafter_by_url.merge(java_files_by_url.reset_index())\n",
    "    irrelevant_commits=commits[commits.classesAfter > commits.changedFile]['commit'].drop_duplicates()\n",
    "    mined=mined[~mined.commit.isin(irrelevant_commits)]\n",
    "    return mined\n",
    "\n",
    "def merge_refs_on_prod_file(mined, manually_reviewed):\n",
    "    dmined=mined\n",
    "    derived=manually_reviewed\n",
    "    derived['prod_file_className']=derived['prod_file'].apply(extract_simple_file_name)\n",
    "    dmined['classNameBefore']=dmined['classesBefore'].apply(extract_simple_class_name)\n",
    "    dmined['classNameAfter']=dmined['classesAfter'].apply(extract_simple_class_name)\n",
    "    m=dmined[dmined.pr_group!='irrelevant'].merge(derived[['url','prod_file_className']].drop_duplicates(), left_on=['url','classNameBefore'], right_on=['url','prod_file_className'])\n",
    "    m=m.append(dmined[dmined.pr_group!='irrelevant'].merge(derived[['url','prod_file_className']].drop_duplicates(), left_on=['url','classNameAfter'], right_on=['url','prod_file_className']))\n",
    "    m=m.append(dmined[dmined.pr_group=='irrelevant'])\n",
    "    m=m.drop_duplicates()\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "if not os.path.isfile('cached_mined_manually.csv'):\n",
    "    mined_manually=get_manually_mined(all_prs, manually_reviewed)\n",
    "    mined_manually.to_csv('cached_mined_manually.csv',index=False)\n",
    "else:\n",
    "    mined_manually=pd.read_csv('cached_mined_manually.csv')\n",
    "mined_manually['project']=mined_manually['url'].apply(extract_project)\n",
    "mined_manually=mined_manually[mined_manually.project.isin(allowed_projects)]\n",
    "mined=mined_manually\n",
    "print(len(mined))\n",
    "mined=exclude_refactored_tests(mined)\n",
    "print(len(mined))\n",
    "mined=mark_reviewed_prs_with_test_pairs(mined, manually_reviewed)\n",
    "print(len(mined))\n",
    "mined=mined.drop_duplicates()\n",
    "print(len(mined))\n",
    "#mined=exclude_too_large_commits(mined, all_prs_with_files)\n",
    "print(len(mined))\n",
    "mined=merge_refs_on_prod_file(mined,manually_reviewed)\n",
    "print(len(mined))\n",
    "means=calc_means_and_counts_by_ref_type(mined,4)\n",
    "def plot_ci_reviewed_means(mined, means, leg_title):\n",
    "    a1=get_refs_per_url(mined, True)\n",
    "    a1['with_tpairs']=True\n",
    "    a2=get_refs_per_url(mined, False)\n",
    "    a2['with_tpairs']=False\n",
    "    a=a1.append(a2)\n",
    "    melted=pd.melt(a.reset_index(), id_vars=['url','with_tpairs'])\n",
    "    means['refactoringTitle']=means['refactoringType'].apply(as_title)\n",
    "    melted['variableTitle']=melted['variable'].apply(as_title)\n",
    "    melted['hue']=melted['with_tpairs'].apply(lambda x: 'Testability relevant PRs (N=' + str(len(a1)) + ')' if x else 'Testability irrelevant PRs (N=' + str(len(a2)) + ')')\n",
    "    plt.figure(figsize=(5, 8))\n",
    "    ax = sns.pointplot(x=\"value\", y=\"variableTitle\", hue=\"hue\",\n",
    "                   data=melted[melted.variableTitle.isin(means.refactoringTitle)],dodge=0.2,\n",
    "                   order=means.sort_values('ratio',ascending=False)['refactoringTitle'],\n",
    "                   markers=[\"o\", \"x\"],capsize=.2,\n",
    "                   linestyles=[\" \", \" \"])\n",
    "    plt.legend(loc='best',bbox_to_anchor=(0.23, 0.85))\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.xlabel('Frequency per PR')\n",
    "    plt.ylabel('')\n",
    "    return ax\n",
    "\n",
    "#image=plot_ci_reviewed_means(mined,means,'Testability relevant PRs')\n",
    "#plot_violins(mined)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ci_generic_means(mined, means, pos_title, neg_title):\n",
    "    a1=get_refs_per_url(mined, True)\n",
    "    a1['with_tpairs']=True\n",
    "    a2=get_refs_per_url(mined, False)\n",
    "    a2['with_tpairs']=False\n",
    "    a=a1.append(a2)\n",
    "    melted=pd.melt(a.reset_index(), id_vars=['url','with_tpairs'])\n",
    "    means['refactoringTitle']=means['refactoringType'].apply(as_title)\n",
    "    means['refactoringTitleSig']=means[['significance','refactoringType']].apply(as_title_sig, axis=1)\n",
    "\n",
    "    melted['variableTitle']=melted['refactoringType'].apply(as_title)\n",
    "    melted['hue']=melted['with_tpairs'].apply(lambda x: pos_title+' (N=' + str(len(a1)) + ')' if x else neg_title + ' (N=' + str(len(a2)) + ')')\n",
    "    plt.figure(figsize=(5, 12))\n",
    "    melted=melted.merge(means[['refactoringTitle','refactoringTitleSig']].drop_duplicates(), left_on='variableTitle',right_on='refactoringTitle')\n",
    "    sel=melted[melted.variableTitle.isin(means.refactoringTitle)]\n",
    "\n",
    "    ax = sns.pointplot(x=\"value\", y=\"refactoringTitleSig\", hue=\"hue\",\n",
    "                   data=melted[melted.variableTitle.isin(means.refactoringTitle)],dodge=0.2,\n",
    "                   order=means.sort_values('ratio',ascending=False)['refactoringTitleSig'],\n",
    "                   markers=[\"o\", \"x\"],capsize=.2,\n",
    "                   linestyles=[\" \", \" \"])\n",
    "    plt.legend(loc='best',bbox_to_anchor=(0.4, 0.75))\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.xlabel('Frequency per PR')\n",
    "    plt.ylabel('')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=plot_ci_generic_means(mined,means,'Testability relevant PRs', 'Testability irrelevant PRs')\n",
    "image.figure.savefig(IMAGES_PATH+'reviewed-ci-means-sig.eps', transparent=False, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_latex_vars(percentage_by_pattern):\n",
    "    for i,row in percentage_by_pattern.iterrows():\n",
    "        print(row['pattern'])\n",
    "        latex_vars['percent'+row['pattern'].replace('_','').lower()]=str(row['percentage']) + '\\\\% '\n",
    "        latex_vars['percent'+row['pattern'].replace('_','').lower()]=str(row['percentage']) + '\\\\% '\n",
    "add_to_latex_vars(count_by_base_pattern)\n",
    "add_to_latex_vars(count_by_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "\n",
    "sel=mined[(mined.pr_group!='irrelevant')&(mined.ref_pattern!='irrelevant')][['refactoringType','ref_pattern']]\n",
    "freq_patterns=sel['ref_pattern'].value_counts()\n",
    "freq_patterns=freq_patterns[freq_patterns>=5]\n",
    "sel=sel[sel.ref_pattern.isin(freq_patterns.index)]\n",
    "sel['refactoringTitle']=sel['refactoringType'].apply(as_title)\n",
    "pam=pd.crosstab(sel.refactoringTitle, sel.ref_pattern)\n",
    "pam=pam.apply(lambda r: 100.0*r/r.sum(),axis=0)\n",
    "pam=pam[[x for x in freq_patterns.index]]\n",
    "fig=plt.figure(figsize = (7,15))\n",
    "#image=sns.heatmap(pam, annot=True, cbar=False, fmt=\".0f\", cmap=cmap)\n",
    "image=sns.heatmap(pam, annot=True, cbar=False, fmt=\".0f\", cmap=\"YlGn\")\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Testability refactoring pattern')\n",
    "plt.xticks(rotation=45,ha='right')\n",
    "image.figure.savefig(IMAGES_PATH+'refminer-patterns-colour2.eps', transparent=False, bbox_inches='tight')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prs_to_mine=pd.read_csv('prs_to_sample2.csv')\n",
    "if not os.path.isfile('cached_30k_mined.csv'):\n",
    "    mined=mined_3k=get_3k_mined(prs_to_mine)\n",
    "    mined.to_csv('cached_30k_mined.csv',index=False)\n",
    "else:\n",
    "    mined=mined_3k=pd.read_csv('cached_30k_mined.csv')\n",
    "print(len(mined))\n",
    "mined=exclude_refactored_tests(mined)\n",
    "print(len(mined))\n",
    "mined=mark_prs_with_test_pairs(mined)\n",
    "print(len(mined))\n",
    "mined=mined.drop_duplicates()\n",
    "print(len(mined))\n",
    "mined=exclude_too_large_commits(mined, all_prs_with_files)\n",
    "print(len(mined))\n",
    "\n",
    "def get_only_allowed_projects_by_url(df):\n",
    "    df['project']=df['url'].apply(extract_project)\n",
    "    return df[df['project'].isin(allowed_projects)]\n",
    "mined=get_only_allowed_projects_by_url(mined)\n",
    "mined_3k=get_only_allowed_projects_by_url(mined_3k)\n",
    "prs_to_mine=get_only_allowed_projects_by_url(prs_to_mine)\n",
    "# limit to 10k\n",
    "prs_to_mine_resampled=prs_to_mine[prs_to_mine.test_pairs==0].sample(10000, random_state=42)\n",
    "prs_to_mine_resampled=prs_to_mine_resampled.append(prs_to_mine[prs_to_mine.test_pairs>0].sample(10000, random_state=42))\n",
    "prs_to_mine=prs_to_mine_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['tpprminedcount']=len(prs_to_mine[prs_to_mine['test_pairs']==0])\n",
    "latex_vars['otherprminedcount']=len(prs_to_mine[prs_to_mine['test_pairs']>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_3k['with_test_pairs']=mined_3k.test_pairs.apply(lambda x: tp_pr_name if x > 0 else 'Other PRs')\n",
    "prs_to_mine['mined_already']=prs_to_mine['url'].isin(set(mined_3k['url']))\n",
    "prs_to_mine['mined_already'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#prs_to_mine=pd.read_csv('prs_to_sample2.csv')\n",
    "#mined_3k=get_3k_mined(prs_to_mine)\n",
    "\n",
    "mined_3k['with_test_pairs']=mined_3k.test_pairs.apply(lambda x: tp_pr_name if x > 0 else 'Other PRs')\n",
    "\n",
    "refactorings_summary=pd.DataFrame()\n",
    "other_pr_name='Other PRs'\n",
    "sel=prs_to_mine\n",
    "sel['with_test_pairs']=sel.test_pairs.apply(lambda x: tp_pr_name if x > 0 else 'Other PRs')\n",
    "g=sel[['with_test_pairs','url']].groupby('with_test_pairs').count().transpose()\n",
    "g.index=['PR count']\n",
    "g.columns.name='Metric'\n",
    "refactorings_summary=refactorings_summary.append(g)\n",
    "\n",
    "\n",
    "sel=mined_3k[['url','with_test_pairs']].drop_duplicates()\n",
    "g=sel.groupby('with_test_pairs').count().transpose()\n",
    "g.index=['PRs with refactorings']\n",
    "g.columns.name='Metric'\n",
    "refactorings_summary=refactorings_summary.append(g)\n",
    "\n",
    "sel=mined_3k[['url','with_test_pairs']]\n",
    "g=sel.groupby('with_test_pairs').count().transpose()\n",
    "g.index=['Mined refactorings']\n",
    "g.columns.name='Metric'\n",
    "refactorings_summary=refactorings_summary.append(g)\n",
    "\n",
    "sel=all_prs.merge(prs_to_mine[['url','mined_already']].drop_duplicates(),on='url')\n",
    "sel['with_test_pairs']=sel.test_pairs.apply(lambda x: tp_pr_name if x > 0 else other_pr_name)\n",
    "sel['Average LOC added per PR']=sel['prod_additions']+sel['test_additions']\n",
    "sel['Average LOC deleted per PR']=sel['prod_deletions']+sel['test_deletions']\n",
    "sel['Average churn LOC per PR']=sel['Average LOC deleted per PR']+sel['Average LOC added per PR']\n",
    "xs=['Average LOC added per PR','Average LOC deleted per PR','Average churn LOC per PR']\n",
    "g=sel.groupby(['url','with_test_pairs']).agg({x:'sum' for x in xs}).groupby('with_test_pairs').agg({x:'mean' for x in xs}).transpose()\n",
    "refactorings_summary=refactorings_summary.append(g)\n",
    "refactorings_summary.loc['Average mined refactorings per PR']=refactorings_summary.loc['Mined refactorings']/refactorings_summary.loc['PRs with refactorings']\n",
    "refactorings_summary=refactorings_summary[[tp_pr_name,other_pr_name]]\n",
    "refactorings_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_idx=refactorings_summary.index.str.contains('Average')\n",
    "table_mined_datasets_lines_a=rows_to_latex(refactorings_summary[~average_idx].astype(int)).split('\\n')\n",
    "table_mined_datasets_lines_b=rows_to_latex(refactorings_summary[average_idx].round(1), grey_idx=1).split('\\n')\n",
    "table_mined_datasets_lines=table_mined_datasets_lines_a+table_mined_datasets_lines_b\n",
    "table_mined_datasets_s='\\n'.join(table_mined_datasets_lines)\n",
    "open('table_mined_datasets.tex','w').write(table_mined_datasets_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=result_by_url.merge(all_prs,on='url')\n",
    "xm=pd.melt(x[['pr_group','prod_additions','test_additions','prod_deletions','test_deletions']], id_vars=['pr_group'])\n",
    "xm=xm.rename(columns={'pr_group':'group','value':'LOC'})\n",
    "image=sns.boxplot(x=\"variable\", y=\"LOC\",\n",
    "            hue=\"group\",\n",
    "            palette='gray',\n",
    "            width=0.6,\n",
    "            data=xm, showfliers = False)\n",
    "image.figure.savefig(IMAGES_PATH+'loc_by_pr_group.eps', transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_means_and_counts_by_ref_type(mined, min_count=5):\n",
    "    w1=agg_data(mined, True)\n",
    "    w2=agg_data(mined, False)\n",
    "    w=w2.merge(w1,on='refactoringType')\n",
    "    with_tests_cnt=len(set(mined[mined.test_pairs>0]['prid']))\n",
    "    without_tests_cnt=len(set(mined[mined.test_pairs==0]['prid']))\n",
    "\n",
    "    w['significance']=w[['with_test_pairs_cnt','without_test_pairs_cnt']].apply(lambda row: round(proportions_ztest([row[0], row[1]], [with_tests_cnt,without_tests_cnt])[1],4), axis=1)\n",
    "\n",
    "    w=w[(w.without_test_pairs_cnt>min_count) & (w.with_test_pairs_cnt > min_count)]\n",
    "    w['ratio']=w['with_test_pairs']/w['without_test_pairs']\n",
    "    return w.sort_values('ratio')\n",
    "means=calc_means_and_counts_by_ref_type(mined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_refactorings_summary(mined_3k):\n",
    "    refactorings_summary=pd.DataFrame()\n",
    "    refactorings_summary.loc['PR count',tp_pr_name]=15000\n",
    "    refactorings_summary.loc['PR count','Other PRs']=15000\n",
    "    mined_3k['with_test_pairs']=mined_3k.test_pairs.apply(lambda x: tp_pr_name if x > 0 else 'Other PRs')\n",
    "    sel=mined_3k[['url','with_test_pairs']].drop_duplicates()\n",
    "    g=sel.groupby('with_test_pairs').count().transpose()\n",
    "    g.index=['PRs with refactorings']\n",
    "    g.columns.name='Metric'\n",
    "    refactorings_summary=refactorings_summary.append(g)\n",
    "\n",
    "    sel=mined_3k[['url','with_test_pairs']]\n",
    "    g=sel.groupby('with_test_pairs').count().transpose()\n",
    "    g.index=['Mined refactorings']\n",
    "    g.columns.name='Metric'\n",
    "    refactorings_summary=refactorings_summary.append(g)\n",
    "\n",
    "    sel=all_prs.merge(prs_to_mine[['url','mined_already']].drop_duplicates(),on='url')\n",
    "    sel['with_test_pairs']=sel.test_pairs.apply(lambda x: tp_pr_name if x > 0 else 'Other PRs')\n",
    "    sel['Average LOC added per PR']=sel['prod_additions']+sel['test_additions']\n",
    "    sel['Average LOC deleted per PR']=sel['prod_deletions']+sel['test_deletions']\n",
    "    sel['Average churn LOC per PR']=sel['Average LOC deleted per PR']+sel['Average LOC added per PR']\n",
    "    xs=['Average LOC added per PR','Average LOC deleted per PR','Average churn LOC per PR']\n",
    "    g=sel.groupby(['url','with_test_pairs']).agg({x:'sum' for x in xs}).groupby('with_test_pairs').agg({x:'mean' for x in xs}).transpose()\n",
    "    refactorings_summary=refactorings_summary.append(g)\n",
    "    refactorings_summary.loc['Average mined refactorings per PR']=refactorings_summary.loc['Mined refactorings']/refactorings_summary.loc['PRs with refactorings']\n",
    "    return refactorings_summary.round(1)[[tp_pr_name,'Other PRs']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['percentrelevantpr']=str(round(len(result_by_url[result_by_url['pr_group']!='irrelevant'])/len(result_by_url)*100)) + '\\\\% '\n",
    "latex_vars['percentirrelevantpr']=str(round(len(result_by_url[result_by_url['pr_group']=='irrelevant'])/len(result_by_url)*100)) + '\\\\% '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in manually_reviewed[['url','title_mask']].drop_duplicates().title_mask.value_counts().iteritems():\n",
    "    print(e)\n",
    "    latex_vars['count'+e[0].replace(' ','').lower()]=e[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_reviewed.reviewer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiwi_sampled=len(manually_reviewed[manually_reviewed.source.isin(['hiwi1','hiwi2','hiwi3'])])\n",
    "latex_vars['hiwisamplereviewedcount']=len(secondary_reviewed[secondary_reviewed.reviewer!='Reviewer1'][['reviewer','url']].drop_duplicates())\n",
    "latex_vars['hiwimasksreviewedcount']=len(secondary_reviewed[secondary_reviewed.reviewer=='Reviewer1'][['reviewer','url']].drop_duplicates())\n",
    "latex_vars['doublereviewedcount']=len(set(double_reviewed.url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rel_reviewed[manually_reviewed.ref_pattern.isin(['extract_method_for_invocation',\n",
    "                                            'extract_method_for_override',\n",
    "                                            'extract_class_for_invocation',\n",
    "                                            'extract_class_for_override',\n",
    "                                            'widen_access_for_invocation',\n",
    "                                            'widen_access_for_override',\n",
    "                                            'create_constructor',\n",
    "                                            'add_constructor_param'\n",
    "                                           ])]['url'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['commonpatternsprcount']=len(set(rel_reviewed[rel_reviewed.ref_pattern.isin(['extract_method_for_invocation',\n",
    "                                            'extract_method_for_override',\n",
    "                                            'extract_class_for_invocation',\n",
    "                                            'extract_class_for_override',\n",
    "                                            'widen_access_for_invocation',\n",
    "                                            'widen_access_for_override',\n",
    "                                            'create_constructor',\n",
    "                                            'add_constructor_param'\n",
    "                                           ])]['url']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_vars['unobviouspatternsprcount']=len(set(manually_reviewed[(manually_reviewed.pr_group!='irrelevant') & (manually_reviewed.ref_pattern=='other')]['url']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in latex_vars.items():\n",
    "    print(k, v)\n",
    "\n",
    "f=open('variables.tex','w')\n",
    "for k, v in latex_vars.items():\n",
    "    f.write('\\\\newcommand{\\\\' + k.replace('_','') + '}[0]{' + str(v) + ' }\\n')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
